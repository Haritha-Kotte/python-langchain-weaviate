{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haritha-Kotte/python-langchain-weaviate/blob/main/meal_planner_weaviate_vectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8aMhjrPu8PY"
      },
      "source": [
        "# Meal Planner\n",
        "\n",
        "In this RAG chain, we are going to:\n",
        "\n",
        "1. Load Dataset from huggingface\n",
        "2. Connect to weaviate\n",
        "3. Create collection in weaviate with the properties\n",
        "4. Ingest embedding data to weaviate collection\n",
        "5. Create the retriever from vector store\n",
        "6. Create a prompt with a template\n",
        "7. Build the RAG chain\n",
        "8. Invoke RAG chain and get the output\n",
        "9. Close connection to weaviate\n",
        "\n",
        "## Prerequisits\n",
        "\n",
        "Before we proceed to do the above steps, we need to\n",
        "\n",
        "- Create an account in weaviate and get the API key for the weaviate cluster\n",
        "- Create an account in huggingface and get the API key for embedding generattion\n",
        "- Create an account in openAI and get the API key for using LLM in the generation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N8P9hNIuZwNV"
      },
      "outputs": [],
      "source": [
        "!pip install weaviate-client datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIxcG_QhPioe"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aOyUfRk0PX2u"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "df = load_dataset(\"Shengtao/recipe\")\n",
        "recipes = df['train']\n",
        "recipes = recipes.select(range(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h438kLcx_K8x"
      },
      "source": [
        "## Connect to Weaviate\n",
        "\n",
        "You have to enter the following credentials in google colab's secrets before connecting to weaviate\n",
        "\n",
        "* WEAVIATE_CLUSTER\n",
        "* WEAVIATE_KEY\n",
        "* HUGGINGFACE_NEW_APIKEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MPdHr-3xQAe4"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import weaviate\n",
        "from weaviate.auth import Auth\n",
        "\n",
        "cluster_url = userdata.get('WEAVIATE_CLUSTER')\n",
        "auth_key = userdata.get('WEAVIATE_KEY')\n",
        "huggingface_new_apikey = userdata.get('HUGGINGFACE_NEW_APIKEY')\n",
        "\n",
        "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=cluster_url,\n",
        "    auth_credentials=Auth.api_key(auth_key),\n",
        "    headers={\"X-HuggingFace-Api-Key\": huggingface_new_apikey},\n",
        "    skip_init_checks=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JLTKcHhAId7"
      },
      "source": [
        "## Create collection in weaviate\n",
        "\n",
        "In the following code block, we are creating a collection in weaviate with the properties from the recipe dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bU3Pw5y_XFV3"
      },
      "outputs": [],
      "source": [
        "import weaviate.classes.config as wvcc\n",
        "\n",
        "properties = [wvcc.Property(\n",
        "                name=col,\n",
        "                data_type=wvcc.DataType.TEXT\n",
        "            ) for col in recipes.column_names if col != \"embedding\"]\n",
        "\n",
        "weaviate_client.collections.delete(\"RecipeV4\")\n",
        "# Note that you can use `client.collections.create_from_dict()` to create a collection from a v3-client-style JSON object\n",
        "collection = weaviate_client.collections.create(\n",
        "    name=\"RecipeV4\",\n",
        "    description=\"A collection to store recipes\",\n",
        "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_huggingface(\n",
        "        model=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "        vectorize_collection_name=True\n",
        "    ),\n",
        "    properties=properties\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRo5VNAFA1_f"
      },
      "source": [
        "## Ingest embedding data to Weaviate\n",
        "\n",
        "In here, we are ingesting data with a rate limited batches as we are triggering huggingface API to generate embeddings as we upload the data and we don't want to hit the rate limit in huggingface.\n",
        "\n",
        "After uploading the data, close the connection to weaviate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zoSAW5fGXYJT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    with weaviate_client.batch.rate_limit(requests_per_minute=10) as batch:  # or <collection>.batch.rate_limit()\n",
        "\n",
        "        for index, row in enumerate(recipes):\n",
        "            data_object = {col: str(row[col]) for col in recipes.column_names}\n",
        "            max_retries = 5\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    batch.add_object(properties=data_object, collection=\"RecipeV4\") #Add object to the collection\n",
        "                    print(\"Data uploaded successfully.\")\n",
        "                    break\n",
        "                except weaviate.exceptions.UnexpectedStatusCodeException as e:\n",
        "                    if '503' in str(e):\n",
        "                        print(f\"Attempt {attempt + 1}: Model is still loading, retrying...\")\n",
        "                        time.sleep(20)  # Wait and retry\n",
        "                    if '429' in str(e):\n",
        "                        # Handle rate limit error\n",
        "                        retry_after = 60  # Retry-After header might be in seconds\n",
        "                        print(f\"Rate limit exceeded. Retrying after {retry_after} seconds...\")\n",
        "                        time.sleep(retry_after)\n",
        "                    else:\n",
        "                        raise  # Raise if it's a different error\n",
        "\n",
        "finally:\n",
        "    weaviate_client.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIdfOxKqBwPD"
      },
      "source": [
        "Now, we have to install necessary packages for RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9avOQO0IfqMk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_weaviate langchain_huggingface langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2099pnWB6-9"
      },
      "source": [
        "## Create the Retriever from the vectorstore\n",
        "\n",
        "Connect to the weaviate client and initialize the vectorstore with the embedding model we want to use for the query embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpxENiRxfiwi"
      },
      "outputs": [],
      "source": [
        "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Mention the embedding model name\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name\n",
        ")\n",
        "\n",
        "weaviate_client.connect()\n",
        "# Initialise the vector store\n",
        "vectorstore = WeaviateVectorStore(client=weaviate_client, index_name=\"RecipeV4\", text_key=\"title\", embedding=embeddings)\n",
        "# Create the retriever to fetch relevant documents based on a query.\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_UDAdE8C7y-"
      },
      "source": [
        "## Create a prompt with the template\n",
        "\n",
        "Created a prompt template that instructs the model to answer questions using retrieved context and to format as mentioned in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCQEDFmtgXKT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Construct a template for the RAG mode\n",
        "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Show in a detailed information list format for the user to prepare the dishes and analyze the nutrition information of the dishes.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0brmt0pwDbdK"
      },
      "source": [
        "## Build the RAG chain\n",
        "\n",
        "We are now going to build a RAG chain that takes the retrieved result as the context and passes it to the prompt which later sends it to the LLM to generate the answer to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vR9s72jIgaQ0"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Connect to OpenAI GPT Model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "# Build the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiAH6FZ7ujkH"
      },
      "source": [
        "## Input the query\n",
        "\n",
        "You can enter the query here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ij4wTglfiSuk"
      },
      "outputs": [],
      "source": [
        "query = \"Recipe with potatoes\" # @param {\"type\":\"string\",\"placeholder\":\"Enter your query\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mY9uur6utRp"
      },
      "source": [
        "## Invoke RAG chain\n",
        "\n",
        "Invoke the RAG chain with the query and print the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TcV0ofAEijl7"
      },
      "outputs": [],
      "source": [
        "output = rag_chain.invoke(query)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qljSsxsu2yH"
      },
      "source": [
        "## Close weaviate connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JorCILpGix49"
      },
      "outputs": [],
      "source": [
        "weaviate_client.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOPePiwTu0JyagshT5+owaw",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
